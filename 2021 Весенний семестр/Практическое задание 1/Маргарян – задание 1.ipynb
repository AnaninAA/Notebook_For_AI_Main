{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание на повторение материала предыдущего семестра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зависимости\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем уникальный seed\n",
    "my_code = \"Маргарян\"\n",
    "seed_limit = 2 ** 32\n",
    "my_seed = int.from_bytes(my_code.encode(), \"little\") % seed_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/russian_demography.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9738be3d9c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Данные загружены отсюда: https://www.kaggle.com/dwdkills/russian-demography\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Читаем данные из файла\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/russian_demography.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/russian_demography.csv'"
     ]
    }
   ],
   "source": [
    "# Данные загружены отсюда: https://www.kaggle.com/dwdkills/russian-demography\n",
    "# Читаем данные из файла\n",
    "example_data = pd.read_csv(\"datasets/russian_demography.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"year\" - год (1990-2017)\n",
    "# \"region\" - название региона\n",
    "# \"npg\" - естественный прирост населения на 1000 человек\n",
    "# \"birth_rate\" - количество рождений на 1000 человек\n",
    "# \"death_rate\" - количество смертей на 1000 человек\n",
    "# \"gdw\" - коэффициент демографической нагрузки на 100 человек (Отношение числа нетрудоспособных к числу трудоспособных).\n",
    "# \"urbanization\" - процент городского населения\n",
    "\n",
    "example_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как список регионов меняется от года к году, в данных есть строки без значений. Удалим их\n",
    "example_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим размер валидационной и тестовой выборок\n",
    "val_test_size = round(0.2*len(example_data))\n",
    "print(val_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим обучающую, валидационную и тестовую выборки\n",
    "random_state = my_seed\n",
    "train_val, test = train_test_split(example_data, test_size=val_test_size, random_state=random_state)\n",
    "train, val = train_test_split(train_val, test_size=val_test_size, random_state=random_state)\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значения в числовых столбцах преобразуем к отрезку [0,1].\n",
    "# Для настройки скалировщика используем только обучающую выборку.\n",
    "columns_to_scale = ['year', 'npg', 'birth_rate', 'death_rate', 'gdw', 'urbanization']\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('numerical', MinMaxScaler(), columns_to_scale)], remainder='passthrough')\n",
    "ct.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем значения, тип данных приводим к DataFrame\n",
    "sc_train = pd.DataFrame(ct.transform(train))\n",
    "sc_test = pd.DataFrame(ct.transform(test))\n",
    "sc_val = pd.DataFrame(ct.transform(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем названия столбцов\n",
    "column_names = columns_to_scale + ['region']\n",
    "sc_train.columns = column_names\n",
    "sc_test.columns = column_names\n",
    "sc_val.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспоминаем алгоритмы решения задачи регрессии: линейную регрессию и метод k ближайших соседей\n",
    "r_models = []\n",
    "\n",
    "# Линейная регрессия\n",
    "# Для использования регуляризации, вместо LinearRegression используем Lasso, Ridge или ElasticNet\n",
    "# Параметр alpha - коэффициент регуляризации для Lasso и Ridge, по умолчанию равен 1\n",
    "# Для ElasticNet, если регуляризация иммет вид a*L1+b*L2, то\n",
    "# параметр alpha = a + b, по умолчанию равен 1\n",
    "# параметр l1_ratio = a / (a + b), по умолчанию равен 0.5\n",
    "r_models.append(LinearRegression())\n",
    "r_models.append(Lasso(alpha=1.0))\n",
    "r_models.append(Ridge(alpha=1.0))\n",
    "r_models.append(ElasticNet(alpha=1.0, l1_ratio=0.5))\n",
    "\n",
    "# K ближайших соседей\n",
    "# Параметр n_neighbors - количество соседей, по умолчания равен 5\n",
    "r_models.append(KNeighborsRegressor(n_neighbors=5))\n",
    "r_models.append(KNeighborsRegressor(n_neighbors=10))\n",
    "r_models.append(KNeighborsRegressor(n_neighbors=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим предикторы и зависимую переменную\n",
    "x_labels = column_names[0:-2]\n",
    "y_labels = ['urbanization']\n",
    "\n",
    "x_train = sc_train[x_labels]\n",
    "x_test = sc_test[x_labels]\n",
    "x_val = sc_val[x_labels]\n",
    "\n",
    "y_train = sc_train[y_labels]\n",
    "y_test = sc_test[y_labels]\n",
    "y_val = sc_val[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели\n",
    "for model in r_models:\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Оценииваем качество работы моделей на валидационной выборке.\n",
    "mses = []\n",
    "for model in r_models:\n",
    "    val_pred = model.predict(x_val)\n",
    "    mse = mean_squared_error(y_val, val_pred)\n",
    "    mses.append(mse)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую модель\n",
    "i_min = mses.index(min(mses))\n",
    "best_r_model = r_models[i_min]\n",
    "best_r_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим ошибку лучшей модели на тестовой выборке.\n",
    "test_pred = best_r_model.predict(x_test)\n",
    "mse = mean_squared_error(y_test, test_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспоминаем алгоритмы решения задачи классификации:\n",
    "# логистическую регрессию, наивный байесовский классификатор и (снова) метод k ближайших соседей\n",
    "c_models = []\n",
    "\n",
    "# Логистическая регрессия\n",
    "# Параметр penalty - тип регуляризации: 'l1', 'l2', 'elasticnet', 'none'}, по умолчанию 'l2'\n",
    "# Для некоторых типов регуляризации доступны не все алгоритмы (параметр solver)\n",
    "# Для elasticnet регуляризации необходимо уазывать параметр l1_ratio (0 - l2, 1 - l1)\n",
    "c_models.append(LogisticRegression(penalty='none', solver='saga'))\n",
    "c_models.append(LogisticRegression(penalty='l1', solver='saga'))\n",
    "c_models.append(LogisticRegression(penalty='l2', solver='saga'))\n",
    "c_models.append(LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga'))\n",
    "c_models.append(LogisticRegression())\n",
    "\n",
    "# Наивный байесовский классификатор\n",
    "# Параметр alpha - параметр сглаживания, по умолчанию равен 1 (сглаживание Лапласа)\n",
    "c_models.append(MultinomialNB(alpha=0.0))\n",
    "c_models.append(MultinomialNB(alpha=0.5))\n",
    "c_models.append(MultinomialNB(alpha=1.0))\n",
    "\n",
    "# K ближайших соседей\n",
    "# Параметр n_neighbors - количество соседей, по умолчания равен 5\n",
    "c_models.append(KNeighborsClassifier(n_neighbors=5))\n",
    "c_models.append(KNeighborsClassifier(n_neighbors=10))\n",
    "c_models.append(KNeighborsClassifier(n_neighbors=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим предикторы и метки классов\n",
    "x_labels = column_names[0:-1]\n",
    "y_labels = ['region']\n",
    "\n",
    "x_train = sc_train[x_labels]\n",
    "x_test = sc_test[x_labels]\n",
    "x_val = sc_val[x_labels]\n",
    "\n",
    "y_train = np.ravel(sc_train[y_labels])\n",
    "y_test = np.ravel(sc_test[y_labels])\n",
    "y_val = np.ravel(sc_val[y_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели\n",
    "for model in c_models:\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценииваем качество работы моделей на валидационной выборке.\n",
    "f1s = []\n",
    "for model in c_models:\n",
    "    val_pred = model.predict(x_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "    f1s.append(f1)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую модель\n",
    "i_min = f1s.index(min(f1s))\n",
    "best_c_model = c_models[i_min]\n",
    "best_c_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим ошибку лучшей модели на тестовой выборке.\n",
    "test_pred = best_c_model.predict(x_test)\n",
    "f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспоминаем алгоритм решения задачи кластеризации - метод k-средних\n",
    "# Параметр n_clusters - количество кластеров, по умолчанию равен 8\n",
    "k_models = []\n",
    "k_models.append(KMeans(n_clusters=5))\n",
    "k_models.append(KMeans(n_clusters=8))\n",
    "k_models.append(KMeans(n_clusters=20))\n",
    "k_models.append(KMeans(n_clusters=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим используемые параметры\n",
    "x_labels = column_names[0:-1]\n",
    "x = pd.concat([sc_train[x_labels], sc_val[x_labels], sc_test[x_labels]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Произведем кластеризацию\n",
    "for model in k_models:\n",
    "    model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим качество результата\n",
    "sils = []\n",
    "for model in k_models:\n",
    "    cluster_labels = model.predict(x)\n",
    "    s = silhouette_score(x, cluster_labels)\n",
    "    sils.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую модель\n",
    "i_min = sils.index(min(sils))\n",
    "best_k_model = k_models[i_min]\n",
    "print(best_k_model)\n",
    "print(sils[i_min])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание №1 - анализ моделей для задачи регрессии\n",
    "# Общий список моделей\n",
    "r_models = [\n",
    "    LinearRegression(),\n",
    "    Lasso(alpha=1.0),\n",
    "    Lasso(alpha=0.5),\n",
    "    Ridge(alpha=1.0),\n",
    "    Ridge(alpha=0.5),\n",
    "    ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "    ElasticNet(alpha=1.0, l1_ratio=0.25),\n",
    "    ElasticNet(alpha=1.0, l1_ratio=0.75),\n",
    "    ElasticNet(alpha=0.5, l1_ratio=0.5),\n",
    "    ElasticNet(alpha=0.5, l1_ratio=0.25),\n",
    "    ElasticNet(alpha=0.5, l1_ratio=0.75),\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    KNeighborsRegressor(n_neighbors=10),\n",
    "    KNeighborsRegressor(n_neighbors=15),\n",
    "    KNeighborsRegressor(n_neighbors=20),\n",
    "    KNeighborsRegressor(n_neighbors=25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор моделей для задания\n",
    "n = 4\n",
    "random.seed(my_seed)\n",
    "my_models1 = random.sample(r_models, n)\n",
    "print(my_models1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузим данные для задачи регрессии\n",
    "data = pd.read_csv(\"datasets/weather.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['water_level'] = data['water_level'].str.replace(',','.')\n",
    "data['precipitation'] = data['precipitation'].str.replace(',','.')\n",
    "data['temperature'] = data['temperature'].str.replace(',','.')\n",
    "data['humidity'] = data['humidity'].str.replace(',','.')\n",
    "data['visibility'] = data['visibility'].str.replace(',','.')\n",
    "data['wind'] = data['wind'].str.replace(',','.')\n",
    "data['weather'] = data['weather'].str.replace(',','.')\n",
    "data['pressure'] = data['pressure'].str.replace(',','.')\n",
    "data['fire'] = data['fire'].str.replace(',','.')                       \n",
    "data['wl_changer'] = data['wl_change'].str.replace(',','.')                       \n",
    "data['temp_change'] = data['temp_change'].str.replace(',','.')                      \n",
    "data['pressure_change'] = data['pressure_change'].str.replace(',','.')                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зависимая переменная для всех одна и та же. Предикторы выбираем случайнм образом.\n",
    "columns = list(data.columns)\n",
    "n_x = 5\n",
    "\n",
    "y_label = ['water_level']\n",
    "x_labels = random.sample(columns[1:], n_x)\n",
    "\n",
    "print(x_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуйте значения всех необходимых параметров к отрезку [0,1].\n",
    "# Решите получившуюся задачу регрессии с помощью выбранных моделей и сравните их эффективность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lb =  x_labels + y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=data[list(all_lb)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим размер валидационной и тестовой выборок\n",
    "val_test_size = round(0.2*len(data))\n",
    "print(val_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим обучающую, валидационную и тестовую выборки\n",
    "random_state = my_seed\n",
    "train_val, test = train_test_split(data, test_size=val_test_size, random_state=random_state)\n",
    "train, val = train_test_split(train_val, test_size=val_test_size, random_state=random_state)\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значения в числовых столбцах преобразуем к отрезку [0,1].\n",
    "# Для настройки скалировщика используем только обучающую выборку.\n",
    "columns_to_scale = all_lb\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('numerical', MinMaxScaler(), columns_to_scale)], remainder='passthrough')\n",
    "ct.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем значения, тип данных приводим к DataFrame\n",
    "sc_train = pd.DataFrame(ct.transform(train))\n",
    "sc_test = pd.DataFrame(ct.transform(test))\n",
    "sc_val = pd.DataFrame(ct.transform(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем названия столбцов\n",
    "column_names = all_lb\n",
    "sc_train.columns = column_names\n",
    "sc_test.columns = column_names\n",
    "sc_val.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sc_train[x_labels]\n",
    "x_test = sc_test[x_labels]\n",
    "x_val = sc_val[x_labels]\n",
    "\n",
    "y_train = sc_train[y_label]\n",
    "y_test = sc_test[y_label]\n",
    "y_val = sc_val[y_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели\n",
    "for model in my_models1:\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценииваем качество работы моделей на валидационной выборке.\n",
    "mses = []\n",
    "for model in my_models1:\n",
    "    val_pred = model.predict(x_val)\n",
    "    mse = mean_squared_error(y_val, val_pred)\n",
    "    mses.append(mse)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую модель # Укажите, какая модель решает задачу лучше других.\n",
    "i_min = mses.index(min(mses))\n",
    "best_my_model = my_models1[i_min]\n",
    "print('Модель ', best_my_model, 'решает задачу лучше других.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим ошибку лучшей модели на тестовой выборке.\n",
    "test_pred = best_r_model.predict(x_test)\n",
    "mse = mean_squared_error(y_test, test_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание №2 - анализ моделей для задачи классификации\n",
    "# Общий список моделей\n",
    "c_models = [\n",
    "    LogisticRegression(penalty='none', solver='saga'),\n",
    "    LogisticRegression(penalty='l1', solver='saga'),\n",
    "    LogisticRegression(penalty='l2', solver='saga'),\n",
    "    LogisticRegression(penalty='elasticnet', l1_ratio=0.25, solver='saga'),\n",
    "    LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga'),\n",
    "    LogisticRegression(penalty='elasticnet', l1_ratio=0.75, solver='saga'),\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(alpha=0.0),\n",
    "    MultinomialNB(alpha=0.25),\n",
    "    MultinomialNB(alpha=0.5),\n",
    "    MultinomialNB(alpha=0.75),\n",
    "    MultinomialNB(alpha=1.0),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    KNeighborsClassifier(n_neighbors=10),\n",
    "    KNeighborsClassifier(n_neighbors=15),\n",
    "    KNeighborsClassifier(n_neighbors=20),\n",
    "    KNeighborsClassifier(n_neighbors=25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор моделей для задания\n",
    "n = 5\n",
    "my_models2 = random.sample(c_models, n)\n",
    "print(my_models2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузим данные для задачи классификации\n",
    "data = pd.read_csv(\"datasets/zoo2.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop(['animal_name'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['animal_name'] = pd.get_dummies(data['animal_name'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метка класса для всех одна и та же. Параметры выбираем случайнм образом.\n",
    "columns = list(data.columns)\n",
    "n_x = 8\n",
    "\n",
    "y_label = ['class_type']\n",
    "x_labels = random.sample(columns[:-1], n_x)\n",
    "all_lb1 =  x_labels + y_label\n",
    "print(x_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуйте значения всех необходимых параметров к отрезку [0,1].\n",
    "# Решите получившуюся задачу классификации с помощью выбранных моделей и сравните их эффективность.\n",
    "# Укажите, какая модель решает задачу лучше других."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[list(all_lb1)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим размер валидационной и тестовой выборок\n",
    "val_test_size = round(0.2*len(data))\n",
    "print(val_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим обучающую, валидационную и тестовую выборки\n",
    "random_state = my_seed\n",
    "train_val, test = train_test_split(data, test_size=val_test_size, random_state=random_state)\n",
    "train, val = train_test_split(train_val, test_size=val_test_size, random_state=random_state)\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значения в числовых столбцах преобразуем к отрезку [0,1].\n",
    "# Для настройки скалировщика используем только обучающую выборку.\n",
    "columns_to_scale = x_labels#all_lb1\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('numerical', MinMaxScaler(), columns_to_scale)], remainder='passthrough')\n",
    "ct.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем значения, тип данных приводим к DataFrame\n",
    "sc_train = pd.DataFrame(ct.transform(train))\n",
    "sc_test = pd.DataFrame(ct.transform(test))\n",
    "sc_val = pd.DataFrame(ct.transform(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем названия столбцов\n",
    "column_names = all_lb1\n",
    "sc_train.columns = column_names\n",
    "sc_test.columns = column_names\n",
    "sc_val.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sc_train[x_labels]\n",
    "x_test = sc_test[x_labels]\n",
    "x_val = sc_val[x_labels]\n",
    "\n",
    "y_train = np.ravel(sc_train[y_label])\n",
    "y_test = np.ravel(sc_test[y_label])\n",
    "y_val = np.ravel(sc_val[y_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модели\n",
    "for model2 in my_models2:\n",
    "    model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценииваем качество работы моделей на валидационной выборке.\n",
    "f1s = []\n",
    "for model in my_models2:\n",
    "    val_pred = model.predict(x_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "    f1s.append(f1)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем лучшую модель # Укажите, какая модель решает задачу лучше других.\n",
    "i_min = f1s.index(min(f1s))\n",
    "best_my_models2 = my_models2[i_min]\n",
    "print('Модель ', best_my_models2, 'решает задачу лучше других.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим ошибку лучшей модели на тестовой выборке.\n",
    "test_pred = best_my_models2.predict(x_test)\n",
    "f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
